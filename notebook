在GF过程中，本文使用叠加自注意机制层数（深度）的方法，进行模型对长期特征的深度语义理解。单层自注意机制由于注意机制层数和权重数量的限制，可以挖掘的注意特征为浅层注意语义信息，表现为
注意力热图的重点分散。双层自注意机制增加了模型的注意语义学习能力，通过自注意叠加的方法使用注意深度语义信息筛选分散的注意热图，从而在长期特征中筛选保留复杂的意图特征，提高注意的效率
和准确性。第二层自注意层从长期特征原图中抽取特征，并对第一层自注意层进行注意特征的特征注意，快速筛选意图热点，从而提升模型预测效果。另外，在叠加注意机制的过程中，由于注意机制的相乘
特性，注意机制层数过多会出现不易训练的情况。
